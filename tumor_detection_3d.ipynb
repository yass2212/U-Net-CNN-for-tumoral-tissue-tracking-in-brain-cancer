{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Détection de Tumeurs Cérébrales 3D — BraTS 2020\n",
    "\n",
    "## Architecture : 3D U-Net pour segmentation\n",
    "\n",
    "**Données** : BraTS2020 — 369 patients, 4 modalités IRM (T1, T1ce, T2, FLAIR), masques de segmentation en 3 classes :\n",
    "- Classe 0 : Noyau nécrotique / tumeur non rehaussée\n",
    "- Classe 1 : Œdème péritumoral\n",
    "- Classe 2 : Tumeur rehaussée (enhancing)\n",
    "\n",
    "**Approche** : Pré-cache des volumes (une seule fois), puis entraînement par patches 3D aléatoires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02576d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports for filesystem operations, randomness, timing and suppressing warnings\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path            # object-oriented filesystem paths, cleaner than os.path\n",
    "from collections import OrderedDict # used to implement an lru cache in the dataset class\n",
    "\n",
    "# scientific computing and data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py                         # read hdf5 files, the format used by brats2020 per-slice archives\n",
    "from scipy.ndimage import zoom as nd_zoom  # spatial downsampling/upsampling during cache creation\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pytorch core modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# automatic mixed precision: uses float16 on gpu to halve memory usage and speed up training\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# progress bars displayed during training and validation loops\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380fa670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select compute device: gpu if available, otherwise fall back to cpu\n",
    "DEVICE  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# automatic mixed precision is only beneficial on gpu (float16 tensor cores)\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # patch size (depth, height, width) fed to the network during training\n",
    "    # larger patches give the model more spatial context but require more vram\n",
    "    PATCH_SIZE   = (64, 160, 160)\n",
    "    # number of feature maps at each encoder level: doubles at each down-sampling step\n",
    "    # [32, 64, 128, 256] gives approximately 11 million parameters\n",
    "    FEATURES     = [32, 64, 128, 256]\n",
    "    # number of samples per forward pass\n",
    "    BATCH_SIZE   = 2\n",
    "    # gradients are accumulated over this many steps before an optimizer update\n",
    "    # effective batch size = BATCH_SIZE * ACCUM_STEPS = 4, without extra vram cost\n",
    "    ACCUM_STEPS  = 2\n",
    "    NUM_WORKERS  = 0  # keep at 0 on windows to avoid multiprocessing deadlocks in jupyter\n",
    "else:\n",
    "    PATCH_SIZE   = (32, 96, 96)\n",
    "    FEATURES     = [16, 32, 64, 128]\n",
    "    BATCH_SIZE   = 1\n",
    "    ACCUM_STEPS  = 1\n",
    "    NUM_WORKERS  = 0\n",
    "\n",
    "# path to the raw brats2020 hdf5 slice files downloaded via kagglehub\n",
    "DATA_DIR   = Path(r'C:/Users/maild/.cache/kagglehub/datasets/awsaf49/brats2020-training-data/versions/3/BraTS2020_training_data/content/data')\n",
    "\n",
    "# folder where model checkpoints are saved (best val dice is kept)\n",
    "SAVE_DIR   = Path('checkpoints')\n",
    "\n",
    "# pre-cache folder: each volume is saved as a single .npy file instead of 155 separate h5 files\n",
    "# this avoids windows defender scanning overhead and reduces load time from ~20s to ~0.05s per volume\n",
    "# storage cost: ~97 mb per volume x 369 volumes = approximately 35 gb\n",
    "CACHE_DIR  = Path('volume_cache_240')\n",
    "CACHE_H    = 240  # full original resolution (no spatial downsampling)\n",
    "CACHE_W    = 240\n",
    "\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "EPOCHS      = 50    # total training epochs; more epochs needed because the model is larger\n",
    "LR          = 2e-4  # peak learning rate reached after the warmup phase\n",
    "WARMUP_EP   = 5     # number of epochs during which lr rises linearly from 0 to LR\n",
    "SEED        = 42\n",
    "NUM_CLASSES = 3     # necrotic core, peritumoral edema, enhancing tumor\n",
    "NUM_SLICES  = 155   # every brats2020 volume contains exactly 155 axial slices\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO   = 0.15  # test ratio is the remainder: 0.15\n",
    "\n",
    "# per-class weights for the weighted dice loss\n",
    "# smaller regions are harder to detect and get higher weights to prevent the model\n",
    "# from ignoring them in favor of the larger, easier edema region\n",
    "# necrosis ~3% of brain voxels, edema ~15%, enhancing tumor ~1%\n",
    "CLASS_WEIGHTS = torch.tensor([1.5, 1.0, 2.0])\n",
    "\n",
    "# fix all random seeds for reproducibility across runs\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# pin_memory speeds up cpu-to-gpu transfers but has no effect without a gpu\n",
    "PIN_MEM = torch.cuda.is_available()\n",
    "\n",
    "print(f'Device      : {DEVICE}')\n",
    "print(f'Patch size  : {PATCH_SIZE}')\n",
    "print(f'Features    : {FEATURES}')\n",
    "print(f'Batch eff.  : {BATCH_SIZE * ACCUM_STEPS}  ({BATCH_SIZE} x {ACCUM_STEPS} accum)')\n",
    "print(f'Cache dir   : {CACHE_DIR.resolve()}')\n",
    "print(f'Cache space : ~{369 * 97 / 1024:.0f} GB estimated (float16 images + uint8 masks)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef34423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 3. EXPLORATION RAPIDE ───────────────────────────────────────────────────\n",
    "meta = pd.read_csv(DATA_DIR / 'meta_data.csv')\n",
    "print('Métadonnées :', meta.shape)\n",
    "print(meta.head(3))\n",
    "print('\\nDistribution target :', meta['target'].value_counts().to_dict())\n",
    "\n",
    "all_vol_ids = sorted(meta['volume'].unique())\n",
    "print(f'Volumes uniques : {len(all_vol_ids)}')\n",
    "\n",
    "with h5py.File(DATA_DIR / 'volume_1_slice_50.h5', 'r') as f:\n",
    "    img  = f['image'][:]\n",
    "    mask = f['mask'][:]\n",
    "print(f'Image H5 brute : {img.shape}  [{img.min():.2f}, {img.max():.2f}]')\n",
    "print(f'Masque H5 brut : {mask.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aed7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4. VISUALISATION D'UNE COUPE ────────────────────────────────────────────\n",
    "MODALITIES   = ['T1', 'T1ce', 'T2', 'FLAIR']\n",
    "TUMOR_LABELS = ['Necrose/Non-rehausse', 'Oedeme', 'Tumeur rehaussee']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('Volume 1 — Coupe 50 | 4 modalites IRM + masques', fontsize=14)\n",
    "\n",
    "for i, mod in enumerate(MODALITIES):\n",
    "    axes[0, i].imshow(img[:, :, i], cmap='gray')\n",
    "    axes[0, i].set_title(mod, fontsize=12); axes[0, i].axis('off')\n",
    "\n",
    "base = (img[:, :, 1] - img[:, :, 1].min()) / (img[:, :, 1].max() - img[:, :, 1].min() + 1e-8)\n",
    "rgb_overlay  = np.stack([base, base, base], axis=-1)\n",
    "class_colors = np.array([[0.9, 0.1, 0.1], [0.1, 0.4, 0.9], [0.1, 0.8, 0.2]])\n",
    "for c in range(3):\n",
    "    region = mask[:, :, c].astype(bool)\n",
    "    rgb_overlay[region] = rgb_overlay[region] * 0.5 + class_colors[c] * 0.5\n",
    "\n",
    "axes[1, 0].imshow(rgb_overlay)\n",
    "axes[1, 0].set_title('T1ce + Masques superposés', fontsize=9); axes[1, 0].axis('off')\n",
    "for c in range(3):\n",
    "    axes[1, c+1].imshow(mask[:, :, c], cmap='hot')\n",
    "    axes[1, c+1].set_title(TUMOR_LABELS[c], fontsize=10); axes[1, c+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_slice.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10931715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-caching converts each patient volume from 155 individual h5 files into two numpy arrays:\n",
    "#   v{id}_img.npy  shape (4, 155, H, W)  dtype float16  stores all 4 mri modalities\n",
    "#   v{id}_mask.npy shape (3, 155, H, W)  dtype uint8    stores the 3 binary tumor masks\n",
    "#\n",
    "# why this is necessary on windows:\n",
    "#   windows defender scans each new file on first read, so opening 155 h5 files\n",
    "#   takes 15 to 25 seconds per volume on a cold filesystem cache.\n",
    "#   loading a single .npy file takes about 0.05 seconds regardless of antivirus.\n",
    "#   this one-time conversion of ~10 minutes saves hours of training time.\n",
    "\n",
    "def precache_volume(vol_id, data_dir, cache_dir, h=128, w=128, overwrite=False):\n",
    "    img_path  = cache_dir / f'v{vol_id}_img.npy'\n",
    "    mask_path = cache_dir / f'v{vol_id}_mask.npy'\n",
    "\n",
    "    # skip if the volume is already cached and overwrite is not requested\n",
    "    if not overwrite and img_path.exists() and mask_path.exists():\n",
    "        return\n",
    "\n",
    "    # compute zoom factors to downsample from the native 240x240 resolution\n",
    "    zoom_h = h / 240.0\n",
    "    zoom_w = w / 240.0\n",
    "    imgs, masks = [], []\n",
    "\n",
    "    # read each of the 155 axial slices and stack them into a volume\n",
    "    for s in range(NUM_SLICES):\n",
    "        path = data_dir / f'volume_{vol_id}_slice_{s}.h5'\n",
    "        with h5py.File(path, 'r') as f:\n",
    "            # image shape: (240, 240, 4) with 4 mri modalities as the last dimension\n",
    "            sl_img  = f['image'][:].astype(np.float32)\n",
    "            # mask shape: (240, 240, 3) with 3 binary tumor regions\n",
    "            sl_mask = f['mask'][:]\n",
    "\n",
    "        # spatially resize the slice if a lower cache resolution was requested\n",
    "        if zoom_h != 1.0 or zoom_w != 1.0:\n",
    "            # bilinear interpolation for images to keep smooth intensity gradients\n",
    "            sl_img  = nd_zoom(sl_img,  [zoom_h, zoom_w, 1], order=1)\n",
    "            # nearest-neighbor for masks to avoid creating fractional label values\n",
    "            sl_mask = nd_zoom(sl_mask, [zoom_h, zoom_w, 1], order=0)\n",
    "\n",
    "        imgs.append(sl_img)\n",
    "        masks.append(sl_mask)\n",
    "\n",
    "    # stack slices: (155, H, W, C) then reorder to pytorch channel-first format (C, 155, H, W)\n",
    "    img_vol  = np.stack(imgs,  axis=0).transpose(3, 0, 1, 2).astype(np.float16)\n",
    "    # float16 halves the disk footprint with negligible precision loss for mri intensities\n",
    "    mask_vol = np.stack(masks, axis=0).transpose(3, 0, 1, 2).astype(np.uint8)\n",
    "\n",
    "    np.save(img_path,  img_vol)\n",
    "    np.save(mask_path, mask_vol)\n",
    "\n",
    "\n",
    "# count how many volumes already have both cache files\n",
    "already_cached = sum(1 for v in all_vol_ids\n",
    "                     if (CACHE_DIR / f'v{v}_img.npy').exists())\n",
    "print(f'Volumes already cached: {already_cached}/{len(all_vol_ids)}')\n",
    "\n",
    "if already_cached < len(all_vol_ids):\n",
    "    print(f'Starting pre-cache for {len(all_vol_ids) - already_cached} volumes...')\n",
    "    print(f'Estimated disk usage: ~{len(all_vol_ids) * 97 / 1024:.0f} GB')\n",
    "    t0 = time.time()\n",
    "    for vol_id in tqdm(all_vol_ids, desc='Pre-cache'):\n",
    "        precache_volume(vol_id, DATA_DIR, CACHE_DIR, h=CACHE_H, w=CACHE_W)\n",
    "    print(f'Pre-cache completed in {(time.time()-t0)/60:.1f} min')\n",
    "else:\n",
    "    print('All volumes cached — ready for training.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 6. BENCHMARK : cache vs H5 ──────────────────────────────────────────────\n",
    "import time\n",
    "\n",
    "# Charger depuis le cache\n",
    "t0 = time.time()\n",
    "for _ in range(5):\n",
    "    _img  = np.load(CACHE_DIR / f'v{all_vol_ids[0]}_img.npy').astype(np.float32)\n",
    "    _mask = np.load(CACHE_DIR / f'v{all_vol_ids[0]}_mask.npy').astype(np.float32)\n",
    "t_cache = (time.time() - t0) / 5\n",
    "\n",
    "# Charger depuis H5 (cold cache impossible de simuler, mais on peut mesurer warm)\n",
    "t0 = time.time()\n",
    "for _ in range(2):\n",
    "    _imgs, _masks = [], []\n",
    "    for s in range(NUM_SLICES):\n",
    "        path = DATA_DIR / f'volume_{all_vol_ids[0]}_slice_{s}.h5'\n",
    "        with h5py.File(path, 'r') as f:\n",
    "            _imgs.append(f['image'][:].astype(np.float32))\n",
    "            _masks.append(f['mask'][:].astype(np.float32))\n",
    "t_h5 = (time.time() - t0) / 2\n",
    "\n",
    "print(f'Chargement cache .npy : {t_cache:.3f}s par volume')\n",
    "print(f'Chargement H5 (warm)  : {t_h5:.2f}s  par volume')\n",
    "print(f'Acceleration          : x{t_h5/t_cache:.0f}')\n",
    "print(f'Image cache shape     : {_img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08830092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset class handles three concerns:\n",
    "#   1. efficient loading via a two-level cache (disk npy then ram lru)\n",
    "#   2. per-channel z-score normalization applied after loading\n",
    "#   3. random patch extraction and on-the-fly data augmentation during training\n",
    "\n",
    "class BraTS3DDataset(Dataset):\n",
    "\n",
    "    def __init__(self, volume_ids, data_dir, cache_dir=None,\n",
    "                 patch_size=None, augment=False, ram_cache_size=4):\n",
    "        self.volume_ids = list(volume_ids)\n",
    "        self.data_dir   = Path(data_dir)\n",
    "        self.cache_dir  = Path(cache_dir) if cache_dir else None\n",
    "        self.patch_size = patch_size    # (D, H, W) or None to return the full volume\n",
    "        self.augment    = augment\n",
    "        # ordered dict used as an lru cache: most-recently-used volumes stay in ram\n",
    "        # avoids reloading the same patient when the dataloader samples it multiple times per epoch\n",
    "        self._ram     = OrderedDict()\n",
    "        self._ram_max = ram_cache_size  # number of volumes to keep simultaneously in ram\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.volume_ids)\n",
    "\n",
    "    def _load_volume(self, vol_id):\n",
    "        # level 1: check ram lru cache first (fastest path, sub-millisecond)\n",
    "        if vol_id in self._ram:\n",
    "            self._ram.move_to_end(vol_id)\n",
    "            return self._ram[vol_id]\n",
    "\n",
    "        # level 2: load from the pre-built .npy cache on disk (~0.05s)\n",
    "        if self.cache_dir is not None:\n",
    "            ip = self.cache_dir / f'v{vol_id}_img.npy'\n",
    "            mp = self.cache_dir / f'v{vol_id}_mask.npy'\n",
    "            if ip.exists() and mp.exists():\n",
    "                img  = np.load(ip).astype(np.float32)   # cast from float16 to float32 for computation\n",
    "                mask = np.load(mp).astype(np.float32)\n",
    "                img  = self._normalize(img)              # z-score normalization applied once after loading\n",
    "                self._put_ram(vol_id, img, mask)\n",
    "                return img, mask\n",
    "\n",
    "        # level 3: slow fallback, reads 155 h5 files sequentially (only if cache is missing)\n",
    "        imgs, masks = [], []\n",
    "        for s in range(NUM_SLICES):\n",
    "            path = self.data_dir / f'volume_{vol_id}_slice_{s}.h5'\n",
    "            with h5py.File(path, 'r') as f:\n",
    "                imgs.append(f['image'][:].astype(np.float32))\n",
    "                masks.append(f['mask'][:].astype(np.float32))\n",
    "        img  = np.stack(imgs,  axis=0).transpose(3, 0, 1, 2)\n",
    "        mask = np.stack(masks, axis=0).transpose(3, 0, 1, 2)\n",
    "        img  = self._normalize(img)\n",
    "        self._put_ram(vol_id, img, mask)\n",
    "        return img, mask\n",
    "\n",
    "    def _put_ram(self, vol_id, img, mask):\n",
    "        self._ram[vol_id] = (img, mask)\n",
    "        # evict the least recently used entry if the cache is full\n",
    "        if len(self._ram) > self._ram_max:\n",
    "            self._ram.popitem(last=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize(img):\n",
    "        # z-score normalization per channel, computed only on non-zero (brain) voxels\n",
    "        # this is critical for multi-modal mri: T1, T1ce, T2 and FLAIR have completely\n",
    "        # different intensity scales across patients, making raw values uninformative for the network\n",
    "        # background voxels (value 0) are excluded from mean/std to avoid shrinking the statistics\n",
    "        # and are restored to exactly 0 after normalization so the network can still detect brain boundaries\n",
    "        out = np.zeros_like(img)\n",
    "        for c in range(img.shape[0]):\n",
    "            ch = img[c]\n",
    "            nz = ch > 0           # non-zero mask (brain region)\n",
    "            if nz.any():\n",
    "                mu      = ch[nz].mean()\n",
    "                std     = ch[nz].std() + 1e-8  # epsilon prevents division by zero in flat regions\n",
    "                out[c]      = (ch - mu) / std\n",
    "                out[c][~nz] = 0.0  # restore background to zero after normalization\n",
    "        return out\n",
    "\n",
    "    def _extract_patch(self, img, mask):\n",
    "        _, D, H, W = img.shape\n",
    "        pd, ph, pw = self.patch_size\n",
    "        # clamp patch size to volume size in case the volume is smaller than the patch\n",
    "        pd = min(pd, D); ph = min(ph, H); pw = min(pw, W)\n",
    "\n",
    "        tumor = (mask > 0).any(axis=0)   # boolean map: True where any tumor class is present\n",
    "\n",
    "        # 60% of the time, center the patch on a random tumor voxel to ensure the network\n",
    "        # sees enough positive examples; pure random sampling would hit tumor only ~0.6% of voxels\n",
    "        if tumor.any() and random.random() > 0.4:\n",
    "            coords = np.argwhere(tumor)\n",
    "            c  = coords[random.randint(0, len(coords) - 1)]\n",
    "            # clip so the patch doesn't go out of bounds\n",
    "            d0 = int(np.clip(c[0] - pd // 2, 0, D - pd))\n",
    "            h0 = int(np.clip(c[1] - ph // 2, 0, H - ph))\n",
    "            w0 = int(np.clip(c[2] - pw // 2, 0, W - pw))\n",
    "        else:\n",
    "            # random patch from anywhere in the volume to prevent overfitting on tumor location\n",
    "            d0 = random.randint(0, max(0, D - pd))\n",
    "            h0 = random.randint(0, max(0, H - ph))\n",
    "            w0 = random.randint(0, max(0, W - pw))\n",
    "\n",
    "        return (img[:, d0:d0+pd, h0:h0+ph, w0:w0+pw],\n",
    "                mask[:, d0:d0+pd, h0:h0+ph, w0:w0+pw])\n",
    "\n",
    "    def _augment(self, img, mask):\n",
    "        # augmentations simulate real acquisition variability and prevent overfitting\n",
    "        # they are only applied during training (augment=True)\n",
    "\n",
    "        # random flips along each spatial axis: cheap and very effective\n",
    "        for ax in [1, 2, 3]:\n",
    "            if random.random() > 0.5:\n",
    "                img  = np.flip(img,  axis=ax).copy()\n",
    "                mask = np.flip(mask, axis=ax).copy()\n",
    "\n",
    "        # random 90-degree rotation in the axial plane (H x W)\n",
    "        # mimics different patient orientations in the scanner\n",
    "        k = random.randint(0, 3)\n",
    "        if k > 0:\n",
    "            img  = np.rot90(img,  k, axes=(2, 3)).copy()\n",
    "            mask = np.rot90(mask, k, axes=(2, 3)).copy()\n",
    "\n",
    "        # random intensity scaling per channel: simulates scanner gain differences\n",
    "        # applied independently to each modality so their relative contrast changes\n",
    "        if random.random() > 0.5:\n",
    "            for c in range(img.shape[0]):\n",
    "                img[c] *= random.uniform(0.85, 1.15)\n",
    "\n",
    "        # global additive intensity shift: simulates bias field variation\n",
    "        if random.random() > 0.5:\n",
    "            img += np.random.uniform(-0.1, 0.1)\n",
    "\n",
    "        # gamma correction: non-linear contrast change that mimics different display windowing\n",
    "        # applied after shifting img to a positive range to avoid complex-number issues\n",
    "        if random.random() > 0.5:\n",
    "            gamma   = random.uniform(0.75, 1.25)\n",
    "            img_min = img.min()\n",
    "            img_pos = img - img_min + 1e-8\n",
    "            img     = np.sign(img_pos) * (np.abs(img_pos) ** gamma) + img_min\n",
    "\n",
    "        # very light gaussian noise: regularization similar to dropout but on the input level\n",
    "        if random.random() > 0.7:\n",
    "            img = img + np.random.normal(0, 0.01, img.shape).astype(np.float32)\n",
    "\n",
    "        return img.astype(np.float32), mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vol_id = self.volume_ids[idx]\n",
    "        img, mask = self._load_volume(vol_id)\n",
    "\n",
    "        if self.patch_size is not None:\n",
    "            img, mask = self._extract_patch(img, mask)\n",
    "        if self.augment:\n",
    "            img, mask = self._augment(img, mask)\n",
    "\n",
    "        # convert to pytorch tensors; .copy() ensures contiguous memory after np.flip / np.rot90\n",
    "        return torch.from_numpy(img.copy()), torch.from_numpy(mask.copy())\n",
    "\n",
    "print('BraTS3DDataset v2 defined.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient-level split: shuffle patient ids and divide into train / val / test subsets\n",
    "# splitting on volume ids (patients) rather than slice ids is critical to prevent data leakage:\n",
    "#   if slices from the same patient appear in both train and val, the model memorizes that patient\n",
    "#   and val dice will be artificially inflated, not reflecting real generalization\n",
    "rng = random.Random(SEED)\n",
    "shuffled_ids = all_vol_ids[:]   # copy so we don't modify the original list\n",
    "rng.shuffle(shuffled_ids)\n",
    "\n",
    "n_train = int(len(shuffled_ids) * TRAIN_RATIO)\n",
    "n_val   = int(len(shuffled_ids) * VAL_RATIO)\n",
    "train_ids = shuffled_ids[:n_train]\n",
    "val_ids   = shuffled_ids[n_train:n_train + n_val]\n",
    "test_ids  = shuffled_ids[n_train + n_val:]   # remainder: ~15% unseen patients\n",
    "print(f'Patients -> Train : {len(train_ids)} | Val : {len(val_ids)} | Test : {len(test_ids)}')\n",
    "\n",
    "# augmentation is only enabled for the training set to prevent distributional shift in validation\n",
    "train_ds = BraTS3DDataset(train_ids, DATA_DIR, CACHE_DIR, PATCH_SIZE, augment=True,  ram_cache_size=4)\n",
    "val_ds   = BraTS3DDataset(val_ids,   DATA_DIR, CACHE_DIR, PATCH_SIZE, augment=False, ram_cache_size=2)\n",
    "test_ds  = BraTS3DDataset(test_ids,  DATA_DIR, CACHE_DIR, PATCH_SIZE, augment=False, ram_cache_size=2)\n",
    "\n",
    "# train loader shuffles every epoch so the model sees patches in a different order each time\n",
    "# val and test loaders use batch_size=1 to avoid mismatching patch shapes across patients\n",
    "# num_workers=0 prevents multiprocessing deadlocks inside jupyter on windows\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEM)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=1, shuffle=False,\n",
    "                          num_workers=0, pin_memory=PIN_MEM)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f'Batches -> Train : {len(train_loader)} | Val : {len(val_loader)} | Test : {len(test_loader)}')\n",
    "\n",
    "# quick sanity check: load one sample and verify shape and tumor voxel fraction\n",
    "t0 = time.time()\n",
    "sample_img, sample_mask = train_ds[0]\n",
    "print(f'First item loaded in {time.time()-t0:.3f}s')\n",
    "print(f'  Image  : {sample_img.shape}  dtype={sample_img.dtype}')\n",
    "print(f'  Mask   : {sample_mask.shape}')\n",
    "print(f'  Tumor voxels : {(sample_mask>0).float().mean().item():.3%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fb9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d u-net architecture\n",
    "# the network follows the encoder-decoder pattern with skip connections\n",
    "# encoder: extracts progressively abstract features at decreasing spatial resolution\n",
    "# decoder: reconstructs the spatial segmentation map using features from the encoder\n",
    "# skip connections: concatenate encoder features into the decoder at each scale,\n",
    "#   preserving fine-grained spatial detail that would otherwise be lost in the bottleneck\n",
    "\n",
    "class DoubleConv3D(nn.Module):\n",
    "    # two consecutive 3d convolutions, each followed by instance normalization and leaky relu\n",
    "    # instance norm is preferred over batch norm for medical imaging because:\n",
    "    #   - it normalizes per-sample rather than per-batch, stable even with batch_size=1\n",
    "    #   - mri images have highly variable intensity distributions across patients\n",
    "    # leaky relu with slope 0.01 avoids dead neurons that plain relu can cause on negative inputs\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            # bias=False because instance norm has its own learnable affine shift (affine=True)\n",
    "            nn.InstanceNorm3d(out_ch, affine=True),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.Conv3d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.InstanceNorm3d(out_ch, affine=True),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.block(x)\n",
    "\n",
    "\n",
    "class Down3D(nn.Module):\n",
    "    # encoder block: halves spatial resolution with maxpool, then doubles feature maps with doubleconv\n",
    "    # maxpool(2) reduces (D, H, W) to (D/2, H/2, W/2) along all three spatial axes\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(nn.MaxPool3d(2), DoubleConv3D(in_ch, out_ch))\n",
    "    def forward(self, x): return self.block(x)\n",
    "\n",
    "\n",
    "class Up3D(nn.Module):\n",
    "    # decoder block: upsamples spatial resolution, then concatenates the corresponding encoder skip,\n",
    "    # and applies doubleconv to merge the two feature sets\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        # transposed convolution (learnable upsampling) doubles spatial dimensions\n",
    "        # it outputs in_ch // 2 feature maps so that after concatenating the skip (also in_ch // 2)\n",
    "        # the combined tensor has in_ch channels, matching the doubleconv input\n",
    "        self.up   = nn.ConvTranspose3d(in_ch, in_ch // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv3D(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        # pad x if its size does not exactly match the skip tensor\n",
    "        # this can happen when the input volume dimensions are not perfectly divisible by 16\n",
    "        pad = [skip.shape[i] - x.shape[i] for i in range(2, 5)]\n",
    "        x = F.pad(x, [0, pad[2], 0, pad[1], 0, pad[0]])\n",
    "        # concatenate along the channel axis and apply the double convolution\n",
    "        return self.conv(torch.cat([skip, x], dim=1))\n",
    "\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    # full 3d u-net with 4 encoding levels and 4 decoding levels\n",
    "    # input:  (B, 4, D, H, W)   four mri modalities stacked as channels\n",
    "    # output: (B, 3, D, H, W)   raw logits for three tumor sub-regions\n",
    "    #   channel 0: necrotic core / non-enhancing tumor\n",
    "    #   channel 1: peritumoral edema\n",
    "    #   channel 2: enhancing tumor (gadolinium uptake on T1ce)\n",
    "\n",
    "    def __init__(self, in_channels=4, out_channels=3, features=None):\n",
    "        super().__init__()\n",
    "        f = features or [16, 32, 64, 128]\n",
    "\n",
    "        # encoder: each level doubles the number of feature maps\n",
    "        self.enc0 = DoubleConv3D(in_channels, f[0])  # full resolution\n",
    "        self.enc1 = Down3D(f[0], f[1])               # resolution / 2\n",
    "        self.enc2 = Down3D(f[1], f[2])               # resolution / 4\n",
    "        self.enc3 = Down3D(f[2], f[3])               # resolution / 8\n",
    "\n",
    "        # bottleneck: deepest representation, smallest spatial size (resolution / 16)\n",
    "        # captures global context at the cost of spatial precision\n",
    "        self.bottleneck = Down3D(f[3], f[3] * 2)\n",
    "\n",
    "        # decoder: mirrors the encoder, each level uses the corresponding encoder skip connection\n",
    "        self.dec3 = Up3D(f[3] * 2, f[3])\n",
    "        self.dec2 = Up3D(f[3],     f[2])\n",
    "        self.dec1 = Up3D(f[2],     f[1])\n",
    "        self.dec0 = Up3D(f[1],     f[0])\n",
    "\n",
    "        # final 1x1x1 convolution maps feature vectors to class logits\n",
    "        # no activation here: sigmoid is applied in the loss and inference functions\n",
    "        self.head = nn.Conv3d(f[0], out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # save encoder outputs for skip connections\n",
    "        s0 = self.enc0(x)\n",
    "        s1 = self.enc1(s0)\n",
    "        s2 = self.enc2(s1)\n",
    "        s3 = self.enc3(s2)\n",
    "        bn = self.bottleneck(s3)\n",
    "        # decode with skip connections from the corresponding encoder level\n",
    "        x  = self.dec3(bn, s3)\n",
    "        x  = self.dec2(x,  s2)\n",
    "        x  = self.dec1(x,  s1)\n",
    "        x  = self.dec0(x,  s0)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "model = UNet3D(4, NUM_CLASSES, FEATURES).to(DEVICE)\n",
    "total_p = sum(p.numel() for p in model.parameters())\n",
    "print(f'Parameters: {total_p:,}')\n",
    "\n",
    "# sanity check: verify the output shape matches the input shape exactly (required for segmentation)\n",
    "with torch.no_grad():\n",
    "    dummy = torch.zeros(1, 4, *PATCH_SIZE, device=DEVICE)\n",
    "    out   = model(dummy)\n",
    "    print(f'Input: {dummy.shape}  Output: {out.shape}')\n",
    "    assert out.shape[2:] == dummy.shape[2:], 'output spatial size must match input'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbdcf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions and evaluation metric\n",
    "#\n",
    "# why dice loss instead of cross-entropy alone?\n",
    "#   tumor voxels represent only ~0.6% of all voxels in a volume.\n",
    "#   cross-entropy would achieve 99.4% accuracy by predicting background everywhere.\n",
    "#   dice loss measures overlap directly, making it class-imbalance robust by design.\n",
    "#\n",
    "# why weighted dice?\n",
    "#   not all three tumor regions are equally hard:\n",
    "#     edema (class 1):          ~15% of brain voxels, large and well-defined\n",
    "#     necrosis (class 0):       ~3%  of brain voxels, irregular texture\n",
    "#     enhancing tumor (class 2):~1%  of brain voxels, clinically most critical for treatment\n",
    "#   without weighting, the model over-focuses on edema and almost ignores the enhancing tumor.\n",
    "#\n",
    "# why focal loss instead of plain bce?\n",
    "#   focal loss down-weights easy-to-classify voxels (background far from tumor)\n",
    "#   so the gradient is dominated by the hard ambiguous border voxels.\n",
    "\n",
    "class WeightedDiceLoss(nn.Module):\n",
    "    def __init__(self, weights=None, smooth=1e-6):\n",
    "        super().__init__()\n",
    "        self.smooth  = smooth\n",
    "        self.weights = weights  # tensor of shape (num_classes,)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # apply sigmoid to convert raw logits to probabilities in [0, 1]\n",
    "        p = torch.sigmoid(logits).flatten(2)   # shape (B, C, N) where N = D*H*W\n",
    "        t = targets.flatten(2)\n",
    "\n",
    "        # compute intersection and the dice score per class per sample\n",
    "        inter = (p * t).sum(2)\n",
    "        dice  = (2 * inter + self.smooth) / (p.sum(2) + t.sum(2) + self.smooth)  # shape (B, C)\n",
    "\n",
    "        # apply per-class weights and average over classes, then over the batch\n",
    "        w    = self.weights.to(logits.device) if self.weights is not None else torch.ones(3, device=logits.device)\n",
    "        loss = 1 - (dice * w.unsqueeze(0)).sum(1) / w.sum()\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class FocalBCELoss(nn.Module):\n",
    "    # focal loss: standard bce multiplied by (1 - p_t)^gamma\n",
    "    # when the model is confident (p_t close to 1), the factor is near 0 so the loss is tiny\n",
    "    # when the model is wrong (p_t close to 0), the factor is near 1 so the full loss applies\n",
    "    # gamma=2 is the standard value from the original focal loss paper (lin et al. 2017)\n",
    "    def __init__(self, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # compute bce per voxel without reduction so we can apply the focal modulation\n",
    "        bce  = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        p_t  = torch.exp(-bce)   # probability of the correct class\n",
    "        loss = ((1 - p_t) ** self.gamma) * bce\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    # combine weighted dice and focal bce with equal weight\n",
    "    # dice handles the global region overlap, focal bce handles local boundary sharpness\n",
    "    def __init__(self, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.dice  = WeightedDiceLoss(weights=class_weights)\n",
    "        self.focal = FocalBCELoss(gamma=2.0)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        return 0.5 * self.dice(logits, targets) + 0.5 * self.focal(logits, targets)\n",
    "\n",
    "\n",
    "def compute_dice(logits, targets, thr=0.5, smooth=1e-6):\n",
    "    # hard dice score: binarize predictions at threshold then measure overlap\n",
    "    # used only for logging; the loss uses soft (probabilistic) dice\n",
    "    p     = (torch.sigmoid(logits) > thr).float().flatten(2)\n",
    "    t     = targets.flatten(2)\n",
    "    inter = (p * t).sum(2)\n",
    "    dice  = (2 * inter + smooth) / (p.sum(2) + t.sum(2) + smooth)\n",
    "    # average over the batch dimension, return one score per class\n",
    "    return dice.mean(0).cpu().numpy()   # shape (num_classes,)\n",
    "\n",
    "\n",
    "criterion = CombinedLoss(class_weights=CLASS_WEIGHTS)\n",
    "print('Loss v2: WeightedDice + FocalBCE  (class weights:', CLASS_WEIGHTS.tolist(), ')')\n",
    "\n",
    "with torch.no_grad():\n",
    "    fl = torch.randn(1, 3, 4, 4, 4)\n",
    "    ft = (torch.rand(1, 3, 4, 4, 4) > 0.85).float()\n",
    "    print(f'Loss sanity check: {criterion(fl, ft).item():.4f}')\n",
    "    print(f'Dice sanity check: {compute_dice(fl, ft)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f75e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation loop functions\n",
    "# keeping them separate from the main loop makes it easier to swap dataloaders\n",
    "# (e.g., running eval on test set) without duplicating code\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_dice = np.zeros(NUM_CLASSES)\n",
    "    t_load     = 0.0   # cumulative time spent waiting for the dataloader\n",
    "\n",
    "    t_iter = time.time()\n",
    "    for batch_idx, (imgs, masks) in enumerate(tqdm(loader, desc='  Train', leave=False)):\n",
    "        t_load += time.time() - t_iter   # time between end of previous iteration and start of this one\n",
    "\n",
    "        imgs  = imgs.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)   # set_to_none frees memory faster than zeroing\n",
    "\n",
    "        if scaler is not None:\n",
    "            # amp forward pass: convolutions run in float16, loss stays in float32\n",
    "            with autocast():\n",
    "                logits = model(imgs)\n",
    "                loss   = criterion(logits, masks)\n",
    "            # scale the loss before backward to prevent gradient underflow in float16\n",
    "            scaler.scale(loss).backward()\n",
    "            # unscale before clipping so the norm threshold is in real gradient units\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(imgs)\n",
    "            loss   = criterion(logits, masks)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_dice += compute_dice(logits.detach(), masks)\n",
    "        t_iter = time.time()\n",
    "\n",
    "    n = max(len(loader), 1)\n",
    "    # print avg load time to detect if data loading is the bottleneck\n",
    "    print(f'    avg load time: {t_load/n:.3f}s/batch', end='')\n",
    "    return total_loss / n, total_dice / n\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, criterion, device):\n",
    "    # no_grad disables gradient computation entirely: saves ~40% of memory and speeds up inference\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_dice = np.zeros(NUM_CLASSES)\n",
    "    for imgs, masks in tqdm(loader, desc='  Val  ', leave=False):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        logits = model(imgs)\n",
    "        total_loss += criterion(logits, masks).item()\n",
    "        total_dice += compute_dice(logits, masks)\n",
    "    n = max(len(loader), 1)\n",
    "    return total_loss / n, total_dice / n\n",
    "\n",
    "print('Train/eval loop functions defined.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training setup\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "# adamw = adam with decoupled weight decay; better generalization than adam with l2 regularization\n",
    "\n",
    "# learning rate schedule: linear warmup followed by cosine annealing\n",
    "# warmup avoids large gradient updates at the start when weights are random and the loss landscape is steep\n",
    "# cosine annealing smoothly decays the lr to lr/100, preventing oscillation near the final minimum\n",
    "def warmup_cosine(epoch):\n",
    "    if epoch < WARMUP_EP:\n",
    "        return (epoch + 1) / WARMUP_EP                           # rise linearly from 0 to LR\n",
    "    t = (epoch - WARMUP_EP) / max(1, EPOCHS - WARMUP_EP)\n",
    "    return 0.01 + 0.5 * (1 - 0.01) * (1 + np.cos(np.pi * t))   # cosine decay to LR * 0.01\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_cosine)\n",
    "\n",
    "# gradscaler manages the loss scaling needed for float16 amp training\n",
    "# amp runs convolutions in float16 (faster, less vram) but keeps master weights in float32\n",
    "scaler = GradScaler() if USE_AMP else None\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_dice': [], 'val_dice': [],\n",
    "           'dice_necrose': [], 'dice_oedeme': [], 'dice_rehausse': []}\n",
    "best_val_dice = -1.0\n",
    "\n",
    "print(f'Training v2 — {EPOCHS} epochs on {DEVICE}')\n",
    "print(f'Warmup: {WARMUP_EP} ep | Effective batch: {BATCH_SIZE * ACCUM_STEPS}')\n",
    "print('─' * 80)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # training loop with gradient accumulation\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_dice = np.zeros(NUM_CLASSES)\n",
    "    t_load     = 0.0\n",
    "\n",
    "    # zero gradients once before the accumulation loop\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    t_iter = time.time()\n",
    "\n",
    "    for step, (imgs, masks) in enumerate(tqdm(train_loader, desc='  Train', leave=False)):\n",
    "        # measure time spent waiting for the dataloader (data loading bottleneck check)\n",
    "        t_load += time.time() - t_iter\n",
    "\n",
    "        imgs  = imgs.to(DEVICE, non_blocking=True)\n",
    "        masks = masks.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        if scaler is not None:\n",
    "            # amp context: forward pass in float16, loss computation in float32\n",
    "            with autocast():\n",
    "                logits = model(imgs)\n",
    "                # divide loss by accum_steps so that accumulated gradients equal one full batch\n",
    "                loss   = criterion(logits, masks) / ACCUM_STEPS\n",
    "            # scale loss to avoid underflow in float16 gradients\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            logits = model(imgs)\n",
    "            loss   = criterion(logits, masks) / ACCUM_STEPS\n",
    "            loss.backward()\n",
    "\n",
    "        # multiply back to log the true (unscaled) loss value\n",
    "        total_loss += loss.item() * ACCUM_STEPS\n",
    "        total_dice += compute_dice(logits.detach(), masks)\n",
    "\n",
    "        # perform optimizer step only every ACCUM_STEPS batches\n",
    "        if (step + 1) % ACCUM_STEPS == 0 or (step + 1) == len(train_loader):\n",
    "            if scaler is not None:\n",
    "                # unscale before clipping so that the clip norm threshold is in true gradient units\n",
    "                scaler.unscale_(optimizer)\n",
    "                # gradient clipping: prevents exploding gradients, keeps training numerically stable\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)  # set_to_none frees memory instead of filling with zeros\n",
    "\n",
    "        t_iter = time.time()\n",
    "\n",
    "    n_tr    = max(len(train_loader), 1)\n",
    "    tr_loss = total_loss / n_tr\n",
    "    tr_dice = total_dice / n_tr\n",
    "    print(f'    avg load time: {t_load/n_tr:.3f}s/batch', end='')\n",
    "\n",
    "    # validation loop (no gradients needed)\n",
    "    model.eval()\n",
    "    vl_loss_sum = 0.0\n",
    "    vl_dice_sum = np.zeros(NUM_CLASSES)\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in tqdm(val_loader, desc='  Val  ', leave=False):\n",
    "            imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "            logits = model(imgs)\n",
    "            vl_loss_sum += criterion(logits, masks).item()\n",
    "            vl_dice_sum += compute_dice(logits, masks)\n",
    "    n_vl    = max(len(val_loader), 1)\n",
    "    vl_loss = vl_loss_sum / n_vl\n",
    "    vl_dice = vl_dice_sum / n_vl\n",
    "\n",
    "    # update lr schedule after each epoch\n",
    "    scheduler.step()\n",
    "\n",
    "    mtr = tr_dice.mean()\n",
    "    mvl = vl_dice.mean()\n",
    "\n",
    "    # log metrics for plotting after training\n",
    "    history['train_loss'].append(tr_loss)\n",
    "    history['val_loss'].append(vl_loss)\n",
    "    history['train_dice'].append(float(mtr))\n",
    "    history['val_dice'].append(float(mvl))\n",
    "    history['dice_necrose'].append(float(vl_dice[0]))\n",
    "    history['dice_oedeme'].append(float(vl_dice[1]))\n",
    "    history['dice_rehausse'].append(float(vl_dice[2]))\n",
    "\n",
    "    # save checkpoint only when val dice improves (keeps only the best model)\n",
    "    flag = ''\n",
    "    if mvl > best_val_dice:\n",
    "        best_val_dice = mvl\n",
    "        torch.save({'epoch': epoch, 'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(), 'val_dice': best_val_dice,\n",
    "                    'features': FEATURES, 'patch_size': PATCH_SIZE},\n",
    "                   SAVE_DIR / 'best_model.pth')\n",
    "        flag = ' <- BEST'\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    lr_cur  = optimizer.param_groups[0]['lr']\n",
    "    print(f'\\nEpoch {epoch:03d}/{EPOCHS} | '\n",
    "          f'Loss {tr_loss:.4f}/{vl_loss:.4f} | '\n",
    "          f'Dice {mtr:.4f}/{mvl:.4f} | '\n",
    "          f'[Nec:{vl_dice[0]:.3f} Oed:{vl_dice[1]:.3f} Enh:{vl_dice[2]:.3f}] | '\n",
    "          f'LR {lr_cur:.1e} | {elapsed:.0f}s{flag}')\n",
    "\n",
    "print('─' * 80)\n",
    "print(f'Best val Dice: {best_val_dice:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c153d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 13. COURBES D'ENTRAÎNEMENT (avec Dice par classe) ───────────────────────\n",
    "ep = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(ep, history['train_loss'], 'b-o', ms=3, label='Train')\n",
    "axes[0].plot(ep, history['val_loss'],   'r-o', ms=3, label='Val')\n",
    "axes[0].set_title('Perte (WeightedDice + FocalBCE)', fontsize=12)\n",
    "axes[0].set_xlabel('Epoque'); axes[0].set_ylabel('Loss')\n",
    "axes[0].legend(); axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Dice moyen\n",
    "axes[1].plot(ep, history['train_dice'], 'b-o', ms=3, label='Train')\n",
    "axes[1].plot(ep, history['val_dice'],   'r-o', ms=3, label='Val')\n",
    "axes[1].set_title('Dice moyen (3 classes)', fontsize=12)\n",
    "axes[1].set_xlabel('Epoque'); axes[1].set_ylabel('Dice')\n",
    "axes[1].set_ylim(0, 1); axes[1].legend(); axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Dice par classe (val uniquement)\n",
    "axes[2].plot(ep, history['dice_necrose'],  'r-o',  ms=3, label='Necrose')\n",
    "axes[2].plot(ep, history['dice_oedeme'],   'b-o',  ms=3, label='Oedeme')\n",
    "axes[2].plot(ep, history['dice_rehausse'], 'g-o',  ms=3, label='Rehausse')\n",
    "axes[2].set_title('Dice Val par classe', fontsize=12)\n",
    "axes[2].set_xlabel('Epoque'); axes[2].set_ylabel('Dice')\n",
    "axes[2].set_ylim(0, 1); axes[2].legend(); axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('3D U-Net v2 — BraTS2020', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Meilleur Dice val :', max(history['val_dice']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c9cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set evaluation: reload the best checkpoint and measure performance on unseen patients\n",
    "# the test set was held out entirely during training and hyperparameter tuning,\n",
    "# so these scores reflect true generalization to new patients\n",
    "\n",
    "# weights_only=False is required to load the full checkpoint dict (contains non-tensor metadata)\n",
    "ckpt = torch.load(SAVE_DIR / 'best_model.pth', map_location=DEVICE, weights_only=False)\n",
    "model.load_state_dict(ckpt['model'])\n",
    "print(f'Model restored (epoch {ckpt[\"epoch\"]}, val_dice={ckpt[\"val_dice\"]:.4f})')\n",
    "\n",
    "# eval_epoch returns mean loss and per-class dice over the full test set\n",
    "te_loss, te_dice = eval_epoch(model, test_loader, criterion, DEVICE)\n",
    "labels = ['Necrosis', 'Edema', 'Enhancing']\n",
    "print(f'\\nTest set results:')\n",
    "print(f'Loss              : {te_loss:.4f}')\n",
    "for c, lab in enumerate(labels):\n",
    "    print(f'Dice {lab:10s} : {te_dice[c]:.4f}')\n",
    "print(f'Mean Dice         : {te_dice.mean():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qualitative prediction check on a single test patient\n",
    "# the goal is to visually confirm that the predicted masks align with the ground truth\n",
    "# and to catch obvious failure modes (e.g., the model predicting no tumor at all)\n",
    "\n",
    "model.eval()\n",
    "test_vol_id = test_ids[0]\n",
    "\n",
    "# load just one patient's volume without augmentation\n",
    "vis_ds      = BraTS3DDataset([test_vol_id], DATA_DIR, CACHE_DIR, patch_size=PATCH_SIZE)\n",
    "img_t, mask_t = vis_ds[0]\n",
    "\n",
    "# run a single forward pass on the central patch\n",
    "with torch.no_grad():\n",
    "    logits = model(img_t.unsqueeze(0).to(DEVICE))   # add batch dimension: (1, 4, D, H, W)\n",
    "    # threshold sigmoid probabilities at 0.5 to get binary predictions\n",
    "    pred   = (torch.sigmoid(logits) > 0.5).squeeze(0).cpu().numpy()\n",
    "\n",
    "img_np  = img_t.numpy()\n",
    "mask_np = mask_t.numpy()\n",
    "mid_d   = img_np.shape[1] // 2   # central axial slice of the patch\n",
    "\n",
    "# one color per tumor class: red = necrosis, blue = edema, green = enhancing\n",
    "colors = np.array([[0.9, 0.1, 0.1], [0.1, 0.4, 0.9], [0.1, 0.8, 0.2]])\n",
    "labels = ['Necrosis', 'Edema', 'Enhancing']\n",
    "\n",
    "# main comparison figure: row 0 = ground truth overlay, row 1 = prediction overlay per class\n",
    "fig2, axes2 = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig2.suptitle(f'Volume {test_vol_id} — GT vs Prediction (central patch slice)', fontsize=13)\n",
    "\n",
    "for c in range(3):\n",
    "    # use the T1ce modality as the background (channel 1): best tumor visibility\n",
    "    base = img_np[1, mid_d]\n",
    "    base = (base - base.min()) / (base.max() - base.min() + 1e-8)\n",
    "\n",
    "    for row, (m2d, ttl) in enumerate([(mask_np[c, mid_d], 'Ground Truth'),\n",
    "                                       (pred[c, mid_d],   'Prediction')]):\n",
    "        rgb = np.stack([base, base, base], axis=-1)\n",
    "        # overlay tumor region with the class color at 65% opacity\n",
    "        if m2d.astype(bool).any():\n",
    "            rgb[m2d.astype(bool)] = rgb[m2d.astype(bool)] * 0.35 + colors[c] * 0.65\n",
    "\n",
    "        # compute dice score for this class across the entire patch volume\n",
    "        dice_c = ((2 * (pred[c] * mask_np[c]).sum() + 1e-6)\n",
    "                  / (pred[c].sum() + mask_np[c].sum() + 1e-6))\n",
    "        axes2[row, c].imshow(rgb)\n",
    "        axes2[row, c].set_title(f'{ttl} — {labels[c]} (Dice={dice_c:.3f})', fontsize=9)\n",
    "        axes2[row, c].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('predictions.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved -> predictions.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17150b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sliding window full-volume inference\n",
    "#\n",
    "# why sliding window instead of a single forward pass?\n",
    "#   a full brats volume is (4, 155, 240, 240).  passing it through the 3d u-net at once\n",
    "#   would require ~40 gb of vram for the activations alone.  instead, we tile the volume\n",
    "#   into overlapping patches, predict each patch, and stitch the probability maps back together.\n",
    "#\n",
    "# why overlap?\n",
    "#   convolutions near patch edges have less contextual information (they see zero padding).\n",
    "#   with 50% overlap, each voxel is covered by multiple patches, and averaging the predictions\n",
    "#   smooths out edge artifacts and improves segmentation quality near boundaries.\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_full_volume(model, vol_id, data_dir, cache_dir, patch_size, device, overlap=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    # load the full volume without any patch cropping\n",
    "    ds = BraTS3DDataset([vol_id], data_dir, cache_dir, patch_size=None)\n",
    "    img, gt_mask = ds[0]   # img: (4, D, H, W)  gt_mask: (3, D, H, W)\n",
    "\n",
    "    D, H, W       = img.shape[1:]\n",
    "    # clamp patch dimensions to the actual volume size (handles edge cases)\n",
    "    pd, ph, pw    = [min(p, s) for p, s in zip(patch_size, (D, H, W))]\n",
    "    # stride = patch_size * (1 - overlap): how far the window moves between positions\n",
    "    sd = max(1, int(pd * (1 - overlap)))\n",
    "    sh = max(1, int(ph * (1 - overlap)))\n",
    "    sw = max(1, int(pw * (1 - overlap)))\n",
    "\n",
    "    # accumulator for summed probabilities; count_map tracks how many patches cover each voxel\n",
    "    pred_sum  = torch.zeros(NUM_CLASSES, D, H, W)\n",
    "    count_map = torch.zeros(1, D, H, W)\n",
    "\n",
    "    # iterate over all patch positions along each spatial axis\n",
    "    for d in range(0, max(1, D - pd + 1), sd):\n",
    "        for h in range(0, max(1, H - ph + 1), sh):\n",
    "            for w in range(0, max(1, W - pw + 1), sw):\n",
    "                # ensure the last patch ends exactly at the volume boundary (no truncation)\n",
    "                d2 = min(d + pd, D); d1 = d2 - pd\n",
    "                h2 = min(h + ph, H); h1 = h2 - ph\n",
    "                w2 = min(w + pw, W); w1 = w2 - pw\n",
    "\n",
    "                patch = img[:, d1:d2, h1:h2, w1:w2].unsqueeze(0).to(device)\n",
    "                # sigmoid converts raw logits to probabilities in [0, 1] per voxel per class\n",
    "                prob  = torch.sigmoid(model(patch)).squeeze(0).cpu()\n",
    "\n",
    "                # add this patch's probabilities to the running sum\n",
    "                pred_sum[:, d1:d2, h1:h2, w1:w2] += prob\n",
    "                count_map[:, d1:d2, h1:h2, w1:w2] += 1\n",
    "\n",
    "    # average over all patches that covered each voxel, then threshold at 0.5\n",
    "    final_prob = pred_sum / count_map.clamp(min=1)\n",
    "    return img.numpy(), gt_mask.numpy(), (final_prob > 0.5).numpy(), final_prob.numpy()\n",
    "\n",
    "\n",
    "print('Running full-volume sliding window inference...')\n",
    "img_full, gt_full, pred_full, prob_full = predict_full_volume(\n",
    "    model, test_ids[0], DATA_DIR, CACHE_DIR, PATCH_SIZE, DEVICE\n",
    ")\n",
    "\n",
    "# report per-class dice on the complete volume (more reliable than patch-level dice)\n",
    "labels = ['Necrosis', 'Edema', 'Enhancing']\n",
    "for c, lab in enumerate(labels):\n",
    "    inter = (pred_full[c] * gt_full[c]).sum()\n",
    "    d = (2 * inter + 1e-6) / (pred_full[c].sum() + gt_full[c].sum() + 1e-6)\n",
    "    print(f'Dice {lab:10s} (full volume) : {d:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b638717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# three-plane anatomical visualization\n",
    "# radiologists examine mri volumes in three standard planes:\n",
    "#   axial    : horizontal cross-section (top-down view), most common for brain imaging\n",
    "#   sagittal : left-right cross-section (side view), shows anterior-posterior extent\n",
    "#   coronal  : front-back cross-section (front view), shows left-right symmetry\n",
    "# displaying all three helps verify that the predicted mask is geometrically consistent\n",
    "# and not just correct in one plane while being wrong in the others\n",
    "\n",
    "T1ce = img_full[1]   # T1 contrast-enhanced modality: best tumor boundary definition\n",
    "D, H, W = T1ce.shape\n",
    "\n",
    "# color coding: red = necrosis, blue = edema, green = enhancing tumor\n",
    "colors = np.array([[0.9, 0.1, 0.1], [0.1, 0.4, 0.9], [0.1, 0.8, 0.2]])\n",
    "\n",
    "# each tuple: (plane name, 2d T1ce slice, ground truth 2d mask, prediction 2d mask)\n",
    "planes = [\n",
    "    ('Axial',    T1ce[D//2, :, :],  gt_full[:, D//2, :, :],  pred_full[:, D//2, :, :]),\n",
    "    ('Sagittal', T1ce[:, H//2, :],  gt_full[:, :, H//2, :],  pred_full[:, :, H//2, :]),\n",
    "    ('Coronal',  T1ce[:, :, W//2],  gt_full[:, :, :, W//2],  pred_full[:, :, :, W//2]),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(13, 13))\n",
    "fig.suptitle(f'Volume {test_ids[0]} — 3 anatomical planes | T1ce / GT / Prediction', fontsize=13)\n",
    "\n",
    "for row, (name, base_2d, gt_2d, pr_2d) in enumerate(planes):\n",
    "    # normalize the grayscale background to [0, 1] for display\n",
    "    b = (base_2d - base_2d.min()) / (base_2d.max() - base_2d.min() + 1e-8)\n",
    "\n",
    "    # column 0: raw T1ce image (no overlay)\n",
    "    axes[row, 0].imshow(b, cmap='gray', aspect='auto')\n",
    "    axes[row, 0].set_title(f'{name} — T1ce', fontsize=10)\n",
    "    axes[row, 0].axis('off')\n",
    "\n",
    "    # columns 1 and 2: overlay all three tumor classes on the T1ce background\n",
    "    for ci, (m2d, ttl) in enumerate([(gt_2d, 'GT'), (pr_2d, 'Pred')]):\n",
    "        rgb = np.stack([b, b, b], axis=-1)\n",
    "        for c in range(3):\n",
    "            # blend tumor regions: 35% original image + 65% class color\n",
    "            if m2d[c].astype(bool).any():\n",
    "                rgb[m2d[c].astype(bool)] = rgb[m2d[c].astype(bool)] * 0.35 + colors[c] * 0.65\n",
    "        axes[row, ci + 1].imshow(rgb, aspect='auto')\n",
    "        axes[row, ci + 1].set_title(f'{name} — {ttl}', fontsize=10)\n",
    "        axes[row, ci + 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('3views_segmentation.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved -> 3views_segmentation.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d15fb3",
   "metadata": {},
   "source": [
    "## Récapitulatif — Pourquoi le pré-cache est indispensable sur CPU\n",
    "\n",
    "| Etape | Sans cache | Avec cache .npy |\n",
    "|-------|-----------|----------------|\n",
    "| Chargement 1 volume (cold) | 15-25s (AV scan 155 fichiers) | ~0.05s |\n",
    "| Chargement 1 volume (warm) | 2.3s | ~0.05s |\n",
    "| Forward+Backward (CPU) | 0.26s | 0.26s |\n",
    "| **Temps/batch total** | **~20s** | **~0.3s** |\n",
    "| **1 époque (258 vol.)** | **~90 min** | **~1.3 min** |\n",
    "\n",
    "Le pré-cache est à faire **une seule fois** (~10-15 min). Ensuite chaque époque prend ~1-2 min.\n",
    "\n",
    "## Architecture 3D U-Net\n",
    "```\n",
    "Entree : (B, 4, D, H, W)  — 4 modalites IRM\n",
    "  enc0 : DoubleConv3D  4  → f[0]       (D,   H,   W  )\n",
    "  enc1 : Down3D  f[0] → f[1]          (D/2, H/2, W/2)\n",
    "  enc2 : Down3D  f[1] → f[2]          (D/4, H/4, W/4)\n",
    "  enc3 : Down3D  f[2] → f[3]          (D/8, H/8, W/8)\n",
    "  bn   : Down3D  f[3] → f[3]*2        (D/16,...      )\n",
    "  dec3 : Up3D + skip enc3\n",
    "  dec2 : Up3D + skip enc2\n",
    "  dec1 : Up3D + skip enc1\n",
    "  dec0 : Up3D + skip enc0\n",
    "  head : Conv3d(f[0], 3, 1x1x1)\n",
    "Sortie : (B, 3, D, H, W)  — logits (Sigmoid -> proba)\n",
    "\n",
    "Perte   : 0.5 x Dice + 0.5 x BCE\n",
    "Optim   : AdamW + CosineAnnealingLR\n",
    "Metrique: Score Dice par classe (BraTS standard)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
